<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<title>Waikato Environment for Knowledge Analysis (WEKA)</title>
<!-- CSS Stylesheet -->
<style>body
{
background: #ededed;
color: #666666;
font: 14px Tahoma, Helvetica, sans-serif;;
margin: 5px 10px 5px 10px;
padding: 0px;
}
</style>

</head>
<body bgcolor="#ededed" text="#666666">


<h2>multiBoostAB: Class for boosting a classifier using the MultiBoosting method.</h2>


<table>
<tr><td valign=top>URL:</td><td width=50></td><td><a href="http://weka.sourceforge.net/doc.packages/multiBoostAB">http://weka.sourceforge.net/doc.packages/multiBoostAB</a></td></tr>
<tr><td valign=top>Author:</td><td width=50></td><td>Shane Butler, Eibe Frank and Len Trigg</td></tr>
<tr><td valign=top>Maintainer:</td><td width=50></td><td>Weka team &#60;wekalist{[at]}list.scms.waikato.ac.nz&#62;</td></tr>
</table>
<p>
<p>Class for boosting a classifier using the MultiBoosting method.<br/><br/>MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction. Using C4.5 as the base learning algorithm, Multi-boosting is demonstrated to produce decision committees with lower error than either AdaBoost or wagging significantly more often than the reverse over a large representative cross-section of UCI data sets. It offers the further advantage over AdaBoost of suiting parallel execution.<br/><br/>For more information, see<br/><br/>Geoffrey I. Webb (2000). MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning. Vol.40(No.2).</p>

<p>All available versions:<br>
<a href="Latest.html">Latest</a><br>
<a href="1.0.1.html">1.0.1</a><br>
<a href="1.0.0.html">1.0.0</a><br>
</body>
</html>
